{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro Gym & Apprentissage par renforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installer le package Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\python311\\lib\\site-packages (from gymnasium) (1.26.0)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\python311\\lib\\site-packages (from gymnasium) (4.8.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "   ---------------------------------------- 0.0/953.9 kB ? eta -:--:--\n",
      "   --------------------------------------  952.3/953.9 kB 30.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  952.3/953.9 kB 30.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 953.9/953.9 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-3.0.0 farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    " !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des package nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "[Gym](https://gymnasium.farama.org/content/basic_usage/) est une boîte à outils pour le développement et la comparaison d'algorithmes d'apprentissage par renforcement. Aucune hypothèse n'est faite sur la structure de l'agent. L'interface est compatible avec toute bibliothèque de calcul numérique, comme TensorFlow ou Theano.\n",
    "\n",
    "La bibliothèque gym est une collection de problèmes de test - environnements - que vous pouvez utiliser pour mettre au point vos algorithmes d'apprentissage par renforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importer un environnement\n",
    "Tous les [envionnements de Gym](https://gymnasium.farama.org/api/env/) s'instencient et se comportent de la même manière. C'est là tout l'intéret de gym.\n",
    "\n",
    "##### Observations\n",
    "Pour prendre des décisions de manière informée, il faut avoir des informations sur l'environnement avec lequel on intéragit. En automatique, on dirait qu'il faut une rétroaction : un système bouclé à de bien meilleures performances qu'un système en boucle ouverte.\n",
    "\n",
    "La fonction step de l'environnement renvoie exactement ce dont nous avons besoin. Elle renvoie quatre valeurs :\n",
    "\n",
    "- observation (list) : Le plus souvent une liste, spécifique à l'environnement, représentant votre observation de l'environnement. Par exemple, les données de pixels d'une caméra, les angles et les vitesses des articulations d'un robot, ou l'état du plateau dans un jeu de société.\n",
    "- récompense (float) : Valeur de la récompense obtenue par l'action précédente. L'échelle varie selon les environnements, mais l'objectif est toujours de cumuler la plus grande la récompense sur un épisode.\n",
    "- done (booléen) : Indique s'il est temps de réinitialiser l'environnement. La plupart des tâches (mais pas toutes) sont divisées en épisodes bien définis. Si done est égale à 1 alors l'épisode est terminé. (Par exemple, peut-être que le poteau a basculé trop loin, ou que vous avez perdu votre dernière vie).\n",
    "- info (dict) : informations de diagnostic utiles pour le débogage. L'agent ne doit pas avoir accès à ces informations pour l'apprentissage.\n",
    "  \n",
    "Il s'agit d'une implémentation de la classique \"boucle agent-environnement\". A chaque pas de temps, l'agent choisit une action, et l'environnement renvoie une observation et une récompense.\n",
    "\n",
    "Chaque épisode débute en appelant reset(), qui renvoie une observation initiale.\n",
    "\n",
    "##### Espace d'observation et d'action\n",
    "Chaque environnement est doté d'un espace d'action et d'un espace d'observation. Ces attributs sont de type gym.Space, et ils décrivent le format des actions et des observations valides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice [Taxi](https://gymnasium.farama.org/environments/toy_text/taxi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Créer l'environnement Taxi (utilisez l'argument render_mode = 'ansi' pour pouvoir afficher l'environnement au fur et a mesure du TD) (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'environnement Taxi avec le rendu ANSI\n",
    "env = gym.make(\"Taxi-v3\", render_mode='ansi')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Utiliser la methode reset pour réinitialiser l'environnement (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, {'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Afficher l'environnement (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Affichage de  l'environnement\n",
    "rendered_state = env.render()\n",
    "\n",
    "# le rendu de l'état\n",
    "print(rendered_state)\n",
    "\n",
    "# Fermeture de l'environnement\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Agent aléatoire. (4 points)\n",
    "\n",
    "Créer une boucle de 5 étapes, à chaque étape piocher une action aléatoire parmi les actions possibles, appliquer l'action puis afficher le nouvel état de l'environnement et la récompense associée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      2\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \n\u001b[1;32m----> 3\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultas de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mÉtape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m result[:\u001b[38;5;241m4\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:293\u001b[0m, in \u001b[0;36mTaxiEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_mask(s)})\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:323\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:399\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, desc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    398\u001b[0m     cell \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size[\u001b[38;5;241m0\u001b[39m], y \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 399\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackground_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m desc[y][x] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m desc[y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][x] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mblit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedian_vert[\u001b[38;5;241m0\u001b[39m], cell)\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    action = env.action_space.sample()  \n",
    "    result = env.step(action) \n",
    "    print(f\"Resultas de l'Étape {i}: {result}\")\n",
    "    observation, reward, done, info = result[:4]\n",
    "    print(f\"Étape {i+1}: Action: {action}, Observation: {observation}, Récompense: {reward}, Terminé: {done}, Info: {info}\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Initialisation de la Q-Table (3 points)\n",
    "   \n",
    "Créer une matrice numpy de zeros de dimensions (nombre d'état possible , nombre d'action possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de la Q-table: (500, 6)\n",
      "Q-table initialisée avec des zéros:\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "print(\"Dimensions de la Q-table:\", q_table.shape)\n",
    "print(\"Q-table initialisée avec des zéros:\")\n",
    "print(q_table)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Initialisation des hyperparametres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.9\n",
    "discount_rate = 0.8\n",
    "epsilon = 1.0\n",
    "decay_rate= 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Créer une boucle de 1000 épisodes avec 100 étapes maximum pour chaque épisodes. (6 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement terminé.\n"
     ]
    }
   ],
   "source": [
    "# pseudo code :\n",
    "\n",
    "#pour 1000 épisodes:\n",
    "    #initialiser env\n",
    "    #pour 99 step:\n",
    "        #affiche l'environnement\n",
    "        #en fonction de la valeur d'epsilon soit:\n",
    "             #choisir une action au hasard (exploration)\n",
    "        #soit:\n",
    "            #choisir l'action avec le meilleur score dans la Qtable\n",
    "        #appliquer l'action en recuperant le nouvel état et la récompense\n",
    "        #actualiser la Qtable\n",
    "        #actualiser l'état de l'environnement avec le nouvel état\n",
    "    #actualiser epsilon\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# pour 1000 épisodes\n",
    "for episode in range(1000):\n",
    "    # affiche l'environnement, intialisation\n",
    "    initial_state_info = env.reset()\n",
    "    state = initial_state_info[0]\n",
    "\n",
    "# pour 99 steps\n",
    "    for step in range(100):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # appliquer l'action en recuperant le nouvel état et la récompense\n",
    "        result = env.step(action)\n",
    "\n",
    "        new_state = result[0]  # Le nouvel état (premier element)\n",
    "        reward = result[1]\n",
    "        done = result[2]\n",
    "\n",
    "        new_state = int(new_state)  # Verifier que c'est un entier\n",
    "\n",
    "        # Mise à jour de la Q-table\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "\n",
    "        state = new_state  # mise à jour de l'état pour la prochaine action\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = max(epsilon - decay_rate, 0.01)\n",
    "\n",
    "\n",
    "# Fermeture de l'environnement\n",
    "env.close()\n",
    "print(\"Entraînement terminé.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Tester l'agent entraîné (4 points)\n",
    "   \n",
    "Réinitialiser l'environemment\n",
    "\n",
    "Créer un boucle permettant de choisir la meilleure action à prendre étant donné l'état de l'environnement en se referant à la Qtable, d'appliquer cette action, la boucle devra se stopper lorsque le taxi reussi sa mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taxi a réussi sa mission!\n",
      "Récompense totale: 5\n"
     ]
    }
   ],
   "source": [
    "# Réinitialiser l'environnement pour commencer un nouvel épisode\n",
    "state = env.reset()[0]\n",
    "\n",
    "done = False\n",
    "total_reward = 0  # Pour garder une trace des récompenses cumulées\n",
    "\n",
    "while not done:\n",
    "    # Choisir la meilleure action à partir de la Q-table pour l'état courant\n",
    "    action = np.argmax(q_table[state, :])\n",
    "\n",
    "    # Appliquer l'action choisie\n",
    "    result = env.step(action)\n",
    "    new_state, reward, done, info = result[:4]\n",
    "\n",
    "    # Ajouter la récompense à la récompense totale\n",
    "    total_reward += reward\n",
    "\n",
    "    # Mettre à jour l'état avec le nouvel état\n",
    "    state = int(new_state)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        print(\"Le taxi a réussi sa mission!\")\n",
    "        print(f\"Récompense totale: {total_reward}\")\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
